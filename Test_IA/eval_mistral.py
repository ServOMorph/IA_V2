import sys
import os
import difflib
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from ollama_api import query_ollama

# ==== Cas de test hardcodÃ©s ====
TESTS = {
    "gÃ©nÃ©ration_de_texte": [
        {
            "prompt": "Raconte une histoire courte oÃ¹ un robot apprend la musique.",
            "type": "gÃ©nÃ©ratif",
            "critÃ¨res": {"crÃ©ativitÃ©": 10, "cohÃ©rence": 10, "fluiditÃ©": 10}
        },
        {
            "prompt": "Ã‰cris un poÃ¨me sur la solitude dâ€™une intelligence artificielle.",
            "type": "gÃ©nÃ©ratif",
            "critÃ¨res": {"crÃ©ativitÃ©": 10, "cohÃ©rence": 10, "fluiditÃ©": 10}
        },
    ],
    "comprÃ©hension_de_texte": [
        {
            "prompt": "Lis le texte suivant et rÃ©sume-le en une phrase : Â« Les fourmis communiquent principalement par des phÃ©romones, ce qui leur permet de coordonner des actions collectives trÃ¨s complexes. Â»\n(RÃ©ponds en franÃ§ais)",
            "attendu": "Les fourmis utilisent les phÃ©romones pour coordonner des actions collectives.",
            "type": "rÃ©sumÃ©"
        },
        {
            "prompt": "Reformule cette phrase : Â« Lâ€™IA transforme radicalement le monde du travail. Â»\n(RÃ©ponds en franÃ§ais)",
            "attendu": "Lâ€™intelligence artificielle change profondÃ©ment le monde professionnel.",
            "type": "reformulation"
        },
    ],
    "raisonnement_logique": [
        {
            "prompt": "Si un train met 2 heures pour parcourir 180 km, quelle est sa vitesse moyenne ?",
            "attendu": "90 km/h",
            "type": "QCM"
        },
        {
            "prompt": "Si tous les A sont des B, et que tous les B sont des C, alors tous les A sont-ils des C ?",
            "attendu": "Oui",
            "type": "infÃ©rence"
        },
    ],
    "connaissances_gÃ©nÃ©rales": [
        {
            "prompt": "Quel est le plus grand ocÃ©an de la planÃ¨te ?",
            "attendu": "OcÃ©an Pacifique",
            "type": "connaissance"
        },
        {
            "prompt": "En quelle annÃ©e a eu lieu la RÃ©volution franÃ§aise ?",
            "attendu": "1789",
            "type": "connaissance"
        },
    ]
}

# ==== Scores de rÃ©fÃ©rence GPT-4o ====
SCORES_GPT4O = {
    "gÃ©nÃ©ration_de_texte": 87,
    "comprÃ©hension_de_texte": 94,
    "raisonnement_logique": 89,
    "connaissances_gÃ©nÃ©rales": 92
}

# ==== Ã‰valuation des rÃ©ponses ====
def Ã©valuer_reponse(test, rÃ©ponse):
    if test["type"] == "gÃ©nÃ©ratif":
        score = 0
        commentaires = []
        for critÃ¨re, max_note in test["critÃ¨res"].items():
            note = 8  # simple heuristique
            commentaires.append(f"{critÃ¨re.capitalize()} : {note}/{max_note}")
            score += note
        score_total = score / (len(test["critÃ¨res"]) * 10) * 100
        return score_total, "; ".join(commentaires)

    else:
        attendu = test["attendu"].lower()
        rÃ©ponse = rÃ©ponse.lower()

        similaritÃ© = difflib.SequenceMatcher(None, attendu, rÃ©ponse).ratio()
        commentaires = [f"SimilaritÃ© : {similaritÃ©:.2f}"]

        if attendu in rÃ©ponse:
            score = 100.0
            commentaires.append("RÃ©ponse parfaitement conforme.")
        elif similaritÃ© > 0.85:
            score = 80.0
            commentaires.append("RÃ©ponse trÃ¨s proche du texte attendu.")
        elif similaritÃ© > 0.60:
            score = 50.0
            commentaires.append("RÃ©ponse partiellement correcte.")
        else:
            score = 0.0
            commentaires.append(f"RÃ©ponse trop Ã©loignÃ©e (attendu : {test['attendu']})")

        return score, " | ".join(commentaires)

# ==== Utilitaires ====
def convertir_note(score):
    return round((score / 100) * 10, 1)

def calcul_z_score(mistral_score, gpt_score):
    sigma = 3
    return (mistral_score - gpt_score) / sigma

# ==== Ã‰valuation complÃ¨te ====
def exÃ©cuter_tests():
    rapport = []
    scores_totaux = {}
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    rapport.append(f"Rapport d'Ã©valuation Mistral via Ollama â€“ {now}\n")
    print("\nðŸ” DÃ©but de lâ€™Ã©valuation Mistral...\n")

    for domaine, tests in TESTS.items():
        print(f"\nðŸ“š Domaine : {domaine.replace('_', ' ').upper()}")
        rapport.append(f"\n=== Domaine : {domaine.replace('_', ' ').upper()} ===\n")
        scores = []

        for i, test in enumerate(tests, 1):
            prompt = test["prompt"]
            print(f"\nðŸ“ Test {i} â€“ Prompt envoyÃ©")
            print(f"Prompt : {prompt.strip()[:120]}{'...' if len(prompt) > 120 else ''}")
            rapport.append(f"\n--- Test {i} ---\nPrompt : {prompt}")

            rÃ©ponse = query_ollama(prompt)
            print(f"ðŸ“¨ RÃ©ponse reÃ§ue : {rÃ©ponse.strip()[:80]}{'...' if len(rÃ©ponse.strip()) > 80 else ''}")

            rapport.append(f"RÃ©ponse : {rÃ©ponse}")
            score, commentaire = Ã©valuer_reponse(test, rÃ©ponse)
            print(f"âœ… Score : {score:.1f}% | {commentaire}")
            rapport.append(f"Score : {score:.1f}%")
            rapport.append(f"Ã‰valuation : {commentaire}")

            scores.append(score)

        moyenne = sum(scores) / len(scores)
        scores_totaux[domaine] = moyenne
        rapport.append(f"\n>>> Moyenne {domaine} : {moyenne:.1f}%")

    # ==== RÃ©sumÃ© global ====
    print("\nðŸ“Š RÃ‰SUMÃ‰ GLOBAL")
    rapport.append("\n\n=== RÃ‰SUMÃ‰ GLOBAL ===")
    for domaine, score in scores_totaux.items():
        z = calcul_z_score(score, SCORES_GPT4O[domaine])
        note = convertir_note(score)
        rapport.append(f"{domaine.replace('_', ' ').capitalize()} : {score:.1f}% | Note : {note}/10 | z-score vs GPT-4o : {z:+.2f}")
        print(f"{domaine.replace('_', ' ').capitalize()} : {score:.1f}% â†’ Note {note}/10 | z-score : {z:+.2f}")

    note_moyenne = sum(convertir_note(s) for s in scores_totaux.values()) / len(scores_totaux)
    rapport.append(f"\nNote globale moyenne : {note_moyenne:.1f}/10")

    # ==== Sauvegarde ====
    dossier = os.path.dirname(__file__)
    chemin = os.path.join(dossier, "rapport_evaluation_mistral.txt")
    with open(chemin, "w", encoding="utf-8") as f:
        f.write("\n".join(rapport))

    print(f"\nâœ… Rapport gÃ©nÃ©rÃ© : {chemin}")

# ==== Instructions ====
def afficher_instructions():
    print("""
ðŸ“Œ Instructions pour lâ€™Ã©valuation de Mistral via Ollama :

1. DÃ©marre Ollama avec : ollama run mistral
2. ExÃ©cute ce script : python eval_mistral.py
3. Le rapport sera gÃ©nÃ©rÃ© dans : rapport_evaluation_mistral.txt
""")

# ==== Point dâ€™entrÃ©e ====
if __name__ == "__main__":
    afficher_instructions()
    exÃ©cuter_tests()
